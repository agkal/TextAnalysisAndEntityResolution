# Description  

The code and process notebook is for analysis and predictive modeling approach to Text Analysis and Entity Resolution. 

# Objective  

Entity resolution is a common, yet difficult problem in data cleaning and integration. This project will demonstrate how we can use Apache Spark to apply powerful and scalable text analysis techniques and perform entity resolution across two datasets of commercial products from Google and Amazon.

# Getting Started  
This assignment can be completed using basic Python, PySpark transformations and actions, and the plotting library matplotlib
Data files for this assignment are from the metric-learning project and can be found at: dbfs/databricks-datasets/cs100/lab3/data-001
The directory contains the following files:
•	Google.csv, the Google Products dataset, named as targets.csv in the repository
•	Amazon.csv, the Amazon dataset, named as sources.csv in the repository
•	Google_small.csv, 200 records sampled from the Google data, subset of targets.csv
•	Amazon_small.csv, 200 records sampled from the Amazon data, subset of sources.csv
•	Amazon_Google_perfectMapping.csv, the "gold standard" mapping, named as mapping.csv in the repository
•	stopwords.txt, a list of common English words

# Prerequisites  

You need to have an account with Databricks cloud and have cluster setup to run the python notebook.
https://community.cloud.databricks.com/login.html

# Files  
Text_Analysis_And_Entity_Resolution.ipynb notebook describes the work and main contributions

